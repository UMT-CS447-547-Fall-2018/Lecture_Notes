{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are a good tool for both classification because they can automatically perform a non-linear change of basis on the data that is optimal for prediction.  They also have the capacity to give us some really useful insights into how they work and why they're so good at generalizing.  Here, we'll construct one from scratch in numpy, in particular one that will solve the binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize']= (16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be generating initial guesses, etc. using random numbers.  To ensure that things go according to plan, let's seed the numpy pseudorandom number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some data to classify.  We can choose any function, but if we want to challenge this thing, it should be something that would fail under normal logistic regression.  For example, let's generate some data that is Bernoulli distributed with $\\theta(x)$ given by two independent bell curves.   I'll generate data from this distribution using a variant of [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1]\n",
    "\n",
    "m_train = 500\n",
    "m_test = 250\n",
    "X_train = np.random.rand(m_train)\n",
    "X_test = np.random.rand(m_test)\n",
    "\n",
    "X_train.sort()\n",
    "X_test.sort()\n",
    "\n",
    "y_pdf = np.exp(-((X_train-0.5)/0.2)**2)# + np.exp(-((X-0.75)/0.1)**2)\n",
    "y_pdf /= y_pdf.max()\n",
    "a = np.random.rand(m_train)\n",
    "y_train = (a<=y_pdf).astype(int)\n",
    "\n",
    "y_pdf = np.exp(-((X_test-0.5)/0.2)**2)# + np.exp(-((X-0.75)/0.1)**2)\n",
    "y_pdf /= y_pdf.max()\n",
    "a = np.random.rand(m_test)\n",
    "y_test = (a<=y_pdf).astype(int)\n",
    "\n",
    "import keras.utils\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 2) \n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train,y_train[:,1],'k.')\n",
    "plt.plot(X_test,y_test[:,1],'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only this data not linearly-seperable, it is also multimodal:  Naive Bayes would be bound to fail because we could not *a priori* determine a sensible probability model for the data.  Logistic regression with the linear basis would also be bound to fail because it can't deal with multiple peaks like this (although we could enrich the basis set instead).  A neural network will allow us to *learn* good basis functions, or how to transform the data to optimize classification.    \n",
    "\n",
    "I confess that I have not implemented a binary classifier: it seemed like a waste of effort when a multiclass method will work fine for the two class case!  As such, we need to make the $T$ matrix (the one hot representation of our class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import keras.models as km\n",
    "import keras.layers as kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import the neural network that I've coded, (the skeleton of) which is available on the course moodle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = km.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a neural network for our problem with the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.add(kl.Dense(2,input_shape=(1,),activation='softmax',use_bias=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is the number of nodes in each layer, so we have one input layer with one node, one hidden layer with four nodes, and one output layer with two nodes.  \n",
    "\n",
    "The second argument is the activation function associated with each.  The input layer has no activation, the second layer is sigmoids, and the third layer is softmax.  \n",
    "\n",
    "The third argument is a boolean value, which states whether to append a bias node for each layer.\n",
    "\n",
    "layer_weight_means_and_stds gives the statistics of the initial guess for weights.\n",
    "\n",
    "We can make predictions with the nn.feed_forward function.  Before we train, we can verify that this, and our gradient code is working properly by computing a finite difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(lr=0.01),\n",
    "              metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same to within 6 places, so good enough.\n",
    "\n",
    "Now, we are all set to perform gradient descent.  We need to define a learning rate $\\eta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.fit(X_train,y_train,batch_size=m_train,epochs=1000,verbose=1,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we iterate for as long as we want.  We'll allow 100000 iterations on the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_model.predict(X_test)\n",
    "plt.plot(X_test,y_pred[:,1],'r.')\n",
    "plt.plot(X_test,y_test[:,1],'k.')\n",
    "plt.show()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a minute to train.  When it's finished, we can plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 2\n",
    "nn_model = km.Sequential()\n",
    "nn_model.add(kl.Dense(n_hidden,input_shape=(1,),use_bias=True,activation='sigmoid'))\n",
    "nn_model.add(kl.Dense(2,use_bias=True,activation='softmax'))\n",
    "nn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "nn_model.fit(X_train,y_train,batch_size=m_train,epochs=5000,verbose=0,validation_data=(X_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good results for a problem that would have thwarted one of our earlier classifiers.  Let's examine what this thing is doing a little bit more deeply.  It's particularly interesting to look at the outputs of the hidden layer, or what basis functions the model decided to transform the data to before classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn_model.predict(X_test)\n",
    "plt.plot(X_test,y_pred[:,1],'r.')\n",
    "plt.plot(X_test,y_test[:,1],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# with a Sequential model\n",
    "get_1st_layer_output = K.function([nn_model.layers[0].input],\n",
    "                                  [nn_model.layers[0].output])\n",
    "layer_output = get_1st_layer_output([X_test.reshape((m_test,1))])[0]\n",
    "\n",
    "get_2nd_layer_input = K.function([nn_model.layers[0].input],\n",
    "                                  [nn_model.layers[1].input])\n",
    "\n",
    "layer_input = get_2nd_layer_input([X_test.reshape((m_test,1))])[0]\n",
    "print(layer_output)\n",
    "print(layer_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These basis functions represent a transform of our data to a new four dimensional space.  It's instructive to see what we get when we add them up and scale them by some weights that we found with gradient descent: the inputs $a_1^{(2)}$ and $a_2^{(2)}$ to the final layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test,y_pred[:,1],'ro')\n",
    "plt.plot(X_test,y_test[:,1],'ko')\n",
    "\n",
    "plt.plot(X_test,layer_output[:,0],'o-')\n",
    "plt.plot(X_test,layer_output[:,1],'o-')\n",
    "#plt.plot(X_test,layer_output[:,2],'o-')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the input to the softmax function is log-probabilities.  That's still what these curves represent.  In particular, you can see that the decision boundaries occur where the log-probabilities for each class are equal.\n",
    "\n",
    "Finally, it's interesting to look at the evolution of the cost function through gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(layer_output[:,0],layer_output[:,1],c=y_test[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_and_b = nn_model.get_weights()\n",
    "w = w_and_b[2]\n",
    "b = w_and_b[3]\n",
    "final_log_probs = layer_output @ w + b\n",
    "plt.plot(X_test,final_log_probs[:,0])\n",
    "plt.plot(X_test,final_log_probs[:,1])\n",
    "plt.scatter(X_test,np.zeros_like(X_test),c=y_pred[:,1]>y_pred[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 256\n",
    "N = 10\n",
    "n_hidden=300\n",
    "epochs = 24\n",
    "\n",
    "rows,cols = 28,28\n",
    "n = rows*cols\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "\n",
    "m_train = X_train.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "X_train = X_train.reshape((m_train,rows*cols))\n",
    "X_test = X_test.reshape((m_test,rows*cols))\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(N,input_shape=(n,),activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "#model.add(kl.Dense(N,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_test,y_test))\n",
    "score = model.evaluate(X_test,y_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "fig,axs = plt.subplots(nrows=1,ncols=10)\n",
    "fig.set_size_inches(16,2)\n",
    "for w,ax in zip(weights[0].T,axs):\n",
    "    ax.imshow(w.reshape((28,28)))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 256\n",
    "N = 10\n",
    "n_hidden_1=512\n",
    "n_hidden_2=512\n",
    "epochs = 24\n",
    "\n",
    "rows,cols = 28,28\n",
    "n = rows*cols\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "\n",
    "m_train = X_train.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "X_train = X_train.reshape((m_train,rows*cols))\n",
    "X_test = X_test.reshape((m_test,rows*cols))\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(n_hidden_1,input_shape=(n,),activation='relu',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(n_hidden_2,activation='relu'))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(N,activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_test,y_test))\n",
    "score = model.evaluate(X_test,y_test,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.imshow(model.get_weights()[0][:,np.random.randint(n_hidden_1)].reshape(28,28))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
