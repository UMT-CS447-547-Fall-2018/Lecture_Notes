{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "Stochastic gradient descent is a useful technique when it would be prohibitively expensive to run all of the training examples at once, or when we wish to update our model in a sequential way.  Here we demonstrate the difference in the convergence properties of batch versus stochastic gradient descent for the simple problem of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset generated from a line with noised added:\n",
    "$$ y = x + 1 + \\epsilon $$\n",
    "$$ x \\in [0,1]. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,21)\n",
    "y = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "plt.plot(x,y,'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our weights to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.,0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cost function (or alternatively negative log-likelihood) is simple least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(w,x,y):\n",
    "    return 1./2.*sum((y - w[0] - w[1]*x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to solve the least squares problem, we take the derivative of this thing with respect to the weights, set them equal to zero.  This produces the normal equations:\n",
    "$$\n",
    "X^T X W = X^T y\n",
    "$$\n",
    "which have an analytical solution.  However, for the purposes of illustration, we can assume that we can't just solve them, and have to use gradient descent.  The gradient for the intercept and the slope of the line we want to fit are \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_0} = -\\sum_{i=1}^m (y_i - w_0 - w_1 x_i)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} = -\\sum_{i=1}^m (y_i - w_0 - w_1 x_i) x_i\n",
    "$$\n",
    "Writing a python function for this gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(w,x,y):\n",
    "    return np.array([-sum(y - w[0] - w[1]*x),-sum((y - w[0] - w[1]*x)*x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can run so-called batch gradient descent, in which we look at all the data points at once.  We'll save our weights at each epoch, so that we can plot them.  A learning rate $\\eta=0.01$ works well for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_batch = [w.copy()]\n",
    "eta = 0.01\n",
    "for i in range(10000):\n",
    "    w -= eta*G(w,x,y)\n",
    "    w_batch.append(w.copy())\n",
    "    \n",
    "w_batch = np.array(w_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our path to the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do so-called stochastic gradient descent, where we compute the gradient based on a single data point sampled at random without replacement from the dataset.  We'll also save the weights at the end of each *epoch*, which is the time period after which all of the datapoints have been observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.,0.])\n",
    "w_stoch = [w.copy()]\n",
    "w_epoch = [w.copy()]\n",
    "eta = 0.01\n",
    "for i in range(10000):\n",
    "    w_epoch.append(w.copy())\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    for j in random_indices:\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "        w -= eta*G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        w_stoch.append(w.copy())\n",
    "\n",
    "w_stoch = np.array(w_stoch)\n",
    "w_epoch = np.array(w_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this on top of the results from batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_epoch[:,0],w_epoch[:,1],'ro')\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some wiggles, but the stochastic value after each epoch falls remarkably close to the batch descent line.  This is even more interesting if we zoom in on the upper right region (near convergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1])\n",
    "plt.plot(w_epoch[:,0],w_epoch[:,1],'ro')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this work?  Let's look at the sum of the individual weight updates in SGD over an epoch.\n",
    "$$\n",
    "\\Delta w_{SGD} = \\eta [\\sum_{i=1}^m (y_i - w_{0,i} - w_{1,i} x_i),\\sum_{i=1}^m (y_i - w_{0,i} - w_{1,i} x_i)x_i]\n",
    "$$\n",
    "Compare this to the update for batch gradient descent\n",
    "$$\n",
    "\\Delta w_{BGD} = \\eta [\\sum_{i=1}^m (y_i - w_{0} - w_{1} x_i),\\sum_{i=1}^m (y_i - w_{0} - w_{1} x_i)x_i]\n",
    "$$\n",
    "You'll note that it's exactly the same, with the exception of the subscripts on the weights.  However, since the weights aren't changing very rapidly (we're taking small steps after all), the resulting updates are very close to identical.\n",
    "\n",
    "These are two end-member options for dealing with gradient descent.  The best solution for the purposes of machine learning lies somewhere in the middle, via a technique called mini-batch gradient descent.  In mini-batch gradient descent, at each epoch we split the data-set into $k$ subsets of a specified size known as the *batch size*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "w = np.array([0.,0.])\n",
    "w_mini = [w.copy()]\n",
    "w_epoch_mini = [w.copy()]\n",
    "eta = 0.01\n",
    "#batch_size = 1 # Stochastic Gradient Descent\n",
    "batch_size = 7 # Mini-batch Gradient Descent\n",
    "#batch_size = 21 # Batch Gradient Descent\n",
    "for i in range(10000):\n",
    "    w_epoch_mini.append(w.copy())\n",
    "    random_indices = np.random.permutation(len(x))\n",
    "    mini_batches = chunks(random_indices,batch_size)\n",
    "    for indices in mini_batches:\n",
    "        x_sample = x[indices]\n",
    "        y_sample = y[indices]\n",
    "        w -= eta*G(w,x_sample,y_sample)\n",
    "        w_mini.append(w.copy())\n",
    "\n",
    "w_mini = np.array(w_mini)\n",
    "w_epoch_mini = np.array(w_epoch_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_mini[:,0],w_mini[:,1],'b-')\n",
    "plt.plot(w_epoch_mini[:,0],w_epoch_mini[:,1],'ro')\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mini-batch gradient descent produces results which are somewhat intermediate between the stochastic and batch versions.  \n",
    "\n",
    "It's worth noting that for this example, stochastic gradient descent takes quite a bit more time to run.  This is because our dataset is relatively small, and the problem we are trying to solve is relatively simple.  However, in large scale problems (think running neural networks over millions of images), it's not possible to fit the training set into memory, and the computation becomes overwhelming.  Simultaneously, in cases where there are many local minimina, SGD may perform better because some local minima may only form for a large number of data points simultaneously.  In this sense, it may also be viewed as a form of *regularization*, because it helps the model avoid overfitting.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "One last popular variant on gradient descent is the inclusion of momentum.  Momentum utilizes the following parameter update:\n",
    "$$\n",
    "\\Delta \\mathbf{w}_i = m \\Delta \\mathbf{w}_{i-1} + (1-m) \\nabla \\mathbf{w}_i\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w}_{i+1} = \\mathbf{w}_i - \\eta \\Delta \\mathbf{w}_i\n",
    "$$\n",
    "This effectively makes the update direction slower to change, and can help to push the model up and out of local minima.  Let's illustrate it's function using stochastic gradient descent (mini-batch size 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.,0.])\n",
    "w_momen = [w.copy()]\n",
    "w_mepoch = [w.copy()]\n",
    "eta = 0.01\n",
    "momentum = 0.9\n",
    "delta_w = 0.0\n",
    "for i in range(10000):\n",
    "    w_mepoch.append(w.copy())\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    for j in random_indices:\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "        delta_w = momentum*delta_w + (1.-momentum)*G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        w -= eta*delta_w\n",
    "        w_momen.append(w.copy())\n",
    "        \n",
    "w_momen = np.array(w_momen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "\n",
    "#plt.xlim(1.1,1.2)\n",
    "#plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum reduces the size of the wiggles due to the stochasticity in stochastic gradient descent.  \n",
    "\n",
    "### RMSprop\n",
    "One final popular variant of gradient descent is called RMSprop, and it is similar to gradient descent with momentum, but with a twist: instead of keeping a running average of the gradient, RMSprop keeps a running average of the squared gradient.  Then, when it comes time to update the weights, it normalizes the gradient by the square-root of this average-squared gradient.  What does this do?  It effectively eliminates the scale of the gradient from the problem, and we only go downhill based on the sign.  The averaging is necessary because the sign of the gradient can jump around alot, so it's better to know generally which direction is down (this is especially true in stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.,0.])\n",
    "w_momen = [w.copy()]\n",
    "w_mepoch = [w.copy()]\n",
    "eta = 0.0001\n",
    "momentum = 0.9\n",
    "delta_w = 0\n",
    "for i in range(10000):\n",
    "    w_mepoch.append(w.copy())\n",
    "    random_indices = np.random.choice(range(len(x)),len(x),replace=False)\n",
    "    for j in random_indices:\n",
    "        x_sample = x[j]\n",
    "        y_sample = y[j]\n",
    "        gradient = G(w,np.array([x_sample]),np.array([y_sample]))\n",
    "        delta_w = momentum*delta_w + (1.-momentum)*gradient**2\n",
    "        w -= eta*gradient/(np.sqrt(delta_w) + 1e-8)\n",
    "        w_momen.append(w.copy())\n",
    "        \n",
    "w_momen = np.array(w_momen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[::1000,0],w_momen[::1000,1],'ro-')\n",
    "\n",
    "#plt.xlim(1.1,1.2)\n",
    "#plt.ylim(0.7,0.8)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_batch[:,0],w_batch[:,1],'go-')\n",
    "plt.plot(w_stoch[:,0],w_stoch[:,1],'b-')\n",
    "plt.plot(w_momen[:,0],w_momen[:,1],'r-')\n",
    "\n",
    "plt.xlim(1.1,1.2)\n",
    "plt.ylim(0.7,0.9)\n",
    "\n",
    "plt.xlabel('Intercept')\n",
    "plt.ylabel('Slope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, RMSprop can be combined with normal momentum.  \n",
    "\n",
    "These are just a few examples of the large scale gradient descent schemes that can be used for general optimization problems, but especially neural networks.  There are many, many other methods (a good overview can be found [here](http://ruder.io/optimizing-gradient-descent/).  However, effectively, these are all just slight variations on the general theme of figuring out which way is down, and going that direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
